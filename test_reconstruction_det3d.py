#!/usr/bin/env python3
"""
æµ‹è¯• MMDetection3D çš„ Det3DDataSample ç»“æ„é‡å»º
"""

import sys
import torch
import os

# Add mmdetection3d to path for accessing data structures
sys.path.insert(0, '/Users/kevinteng/src/kevinteng525/open-mmlab/mmdetection3d')
sys.path.insert(0, '/Users/kevinteng/src/kevinteng525/open-mmlab/refined')

# Import InputFlattener from improved_exporter.py
from improved_exporter import InputFlattener

# æ¨¡æ‹Ÿ InstanceData ç±»
class MockInstanceData:
    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            setattr(self, key, value)

# æ¨¡æ‹Ÿ PointData ç±»
class MockPointData:
    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            setattr(self, key, value)

# æ¨¡æ‹Ÿ Det3DDataSample ç±»
class MockDet3DDataSample:
    def __init__(self):
        self.gt_instances_3d = MockInstanceData()
        self.gt_instances = MockInstanceData()
        self.gt_pts_seg = MockPointData()


def test_case_det3d_data_sample():
    """æµ‹è¯• Det3DDataSample ç»“æ„"""
    print("\n[Test] Det3DDataSample ç»“æ„")
    print("-" * 60)

    # åˆ›å»ºå¤æ‚çš„ MMDetection3D æ•°æ®æ ·æœ¬
    sample = MockDet3DDataSample()

    # è®¾ç½® 3D å®ä¾‹æ•°æ®
    sample.gt_instances_3d = MockInstanceData(
        bboxes_3d=torch.randn(5, 7),
        labels_3d=torch.randint(0, 10, (5,)),
        scores_3d=torch.rand(5)
    )

    # è®¾ç½® 2D å®ä¾‹æ•°æ®
    sample.gt_instances = MockInstanceData(
        bboxes=torch.randn(5, 4),
        labels=torch.randint(0, 10, (5,)),
        scores=torch.rand(5)
    )

    # è®¾ç½®ç‚¹äº‘åˆ†å‰²æ•°æ®
    sample.gt_pts_seg = MockPointData(
        pts_semantic_mask=torch.randint(0, 20, (1000,)),
        pts_instance_mask=torch.randint(0, 50, (1000,))
    )

    # åˆ›å»ºåŒ…å« Det3DDataSample çš„å®Œæ•´è¾“å…¥
    input_data = {
        'inputs': {
            'voxels': torch.randn(1000, 20, 5),
            'num_points': torch.randint(1, 20, (1000,)),
            'coors': torch.randint(0, 100, (1000, 3)),
        },
        'data_samples': [sample],
        'batch_input_shape': (960, 1280),
        'device': 'cuda:0'
    }

    print("åŸå§‹æ•°æ®ç»“æ„:")
    print(f"  inputs: {list(input_data['inputs'].keys())}")
    print(f"  data_samples: {len(input_data['data_samples'])} ä¸ªæ ·æœ¬")
    if input_data['data_samples']:
        sample = input_data['data_samples'][0]
        print(f"    sample.gt_instances_3d.bboxes_3d: {sample.gt_instances_3d.bboxes_3d.shape}")
        print(f"    sample.gt_instances_3d.labels_3d: {sample.gt_instances_3d.labels_3d.shape}")
        print(f"    sample.gt_instances.bboxes: {sample.gt_instances.bboxes.shape}")
        print(f"    sample.gt_pts_seg.pts_semantic_mask: {sample.gt_pts_seg.pts_semantic_mask.shape}")

    # ä½¿ç”¨ improved_exporter ä¸­çš„ InputFlattener
    flattener = InputFlattener()
    flat_tensors = flattener.analyze_and_flatten(input_data)

    print(f"\næå–çš„å¼ é‡:")
    for i, info in enumerate(flattener.tensor_info):
        print(f"  [{i}] {info['path']}: {info['shape']}")

    # é‡å»ºæ•°æ®
    reconstructed = flattener.reconstruct_inputs(flat_tensors)

    # éªŒè¯é‡å»ºç»“æœ
    print("\néªŒè¯é‡å»ºç»“æœ:")
    success = True

    # æ£€æŸ¥åŸºæœ¬è¾“å…¥
    if 'inputs' in reconstructed:
        for key in ['voxels', 'num_points', 'coors']:
            if key in reconstructed['inputs']:
                orig = input_data['inputs'][key]
                recon = reconstructed['inputs'][key]
                if torch.allclose(orig, recon):
                    print(f"  âœ“ inputs.{key} é‡å»ºæˆåŠŸ")
                else:
                    print(f"  âœ— inputs.{key} é‡å»ºå¤±è´¥")
                    success = False
            else:
                print(f"  âœ— inputs.{key} ç¼ºå¤±")
                success = False
    else:
        print("  âœ— inputs ç¼ºå¤±")
        success = False

    print(f"\næµ‹è¯•ç»“æœ: {'âœ“ æˆåŠŸ' if success else 'âœ— å¤±è´¥'}")
    return success


def test_nested_mixed_types():
    """æµ‹è¯•æ··åˆç±»å‹çš„åµŒå¥—ç»“æ„"""
    print("\n[Test] æ··åˆç±»å‹åµŒå¥—ç»“æ„")
    print("-" * 60)

    # åˆ›å»ºåŒ…å«å¼ é‡å’Œéå¼ é‡çš„æ··åˆç»“æ„
    original_data = {
        'model_inputs': {
            'points': [torch.randn(100, 5), torch.randn(200, 5)],  # å¼ é‡åˆ—è¡¨
            'images': {
                'front': torch.randn(3, 224, 224),
                'back': torch.randn(3, 224, 224),
                'metadata': {
                    'camera_ids': ['front', 'back'],  # éå¼ é‡
                    'timestamp': '2023-01-01'  # éå¼ é‡
                }
            }
        },
        'model_config': {
            'voxel_size': [0.1, 0.1, 0.2],  # éå¼ é‡
            'point_cloud_range': [-50, -50, -5, 50, 50, 3]  # éå¼ é‡
        }
    }

    flattener = InputFlattener()
    flat_tensors = flattener.analyze_and_flatten(original_data)

    print(f"æå–çš„å¼ é‡:")
    for i, info in enumerate(flattener.tensor_info):
        print(f"  [{i}] {info['path']}: {info['shape']}")

    # é‡å»ºæ•°æ®
    reconstructed = flattener.reconstruct_inputs(flat_tensors)

    # éªŒè¯é‡å»ºç»“æœ
    print("\néªŒè¯é‡å»ºç»“æœ:")
    success = True

    # æ£€æŸ¥å¼ é‡éƒ¨åˆ†
    if 'model_inputs' in reconstructed:
        # æ£€æŸ¥ points åˆ—è¡¨
        if 'points' in reconstructed['model_inputs']:
            points_recon = reconstructed['model_inputs']['points']
            if isinstance(points_recon, list) and len(points_recon) == 2:
                for i, (orig, recon) in enumerate(zip(original_data['model_inputs']['points'], points_recon)):
                    if torch.allclose(orig, recon):
                        print(f"  âœ“ model_inputs.points[{i}] é‡å»ºæˆåŠŸ")
                    else:
                        print(f"  âœ— model_inputs.points[{i}] é‡å»ºå¤±è´¥")
                        success = False
            else:
                print("  âœ— model_inputs.points ç»“æ„ä¸æ­£ç¡®")
                success = False
        else:
            print("  âœ— model_inputs.points ç¼ºå¤±")
            success = False

        # æ£€æŸ¥ images å­—å…¸
        if 'images' in reconstructed['model_inputs']:
            images_recon = reconstructed['model_inputs']['images']
            for view in ['front', 'back']:
                if view in images_recon:
                    if torch.allclose(original_data['model_inputs']['images'][view], images_recon[view]):
                        print(f"  âœ“ model_inputs.images.{view} é‡å»ºæˆåŠŸ")
                    else:
                        print(f"  âœ— model_inputs.images.{view} é‡å»ºå¤±è´¥")
                        success = False
                else:
                    print(f"  âœ— model_inputs.images.{view} ç¼ºå¤±")
                    success = False
        else:
            print("  âœ— model_inputs.images ç¼ºå¤±")
            success = False
    else:
        print("  âœ— model_inputs ç¼ºå¤±")
        success = False

    print(f"\næµ‹è¯•ç»“æœ: {'âœ“ æˆåŠŸ' if success else 'âœ— å¤±è´¥'}")
    return success


def main():
    print("=" * 60)
    print("InputFlattener MMDetection3D æµ‹è¯•")
    print("ä½¿ç”¨ improved_exporter.py ä¸­çš„ InputFlattener")
    print("=" * 60)

    results = []
    results.append(test_case_det3d_data_sample())
    results.append(test_nested_mixed_types())

    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“:")
    passed = sum(results)
    total = len(results)
    print(f"é€šè¿‡: {passed}/{total}")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰ MMDetection3D æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
    print("=" * 60)


if __name__ == '__main__':
    main()